{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Tabelog Data Collection"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e5479c65352a1ab"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import inspect\n",
    "import urllib.request\n",
    "import http.client\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import pandas as pd\n",
    "import tqdm\n",
    "import time"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-04-12T21:26:13.112166Z",
     "start_time": "2025-04-12T21:26:12.294289Z"
    }
   },
   "id": "626395d738d7482c",
   "execution_count": 2
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Tabelog Restaurant Data"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "71502eec368e7a1d"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "class Tabelog:\n",
    "    \"\"\"\n",
    "    Tabelog scraper for score, review count, daytime and nighttime price, photo count, like count, and bookmark count.\n",
    "    Credit to https://qiita.com/toshiyuki_tsutsui/items/f143946944a428ed105b for setting the foundation of this scraper. ありがとうございます。ദ്ദി(｡•̀ ,<)~✩‧₊\n",
    "    \"\"\"\n",
    "    def __init__(self, base_url, test_mode=False, p_area=\"東京都内\", genre=\"ramen\", begin_page=1, end_page=10):\n",
    "        \"\"\"\n",
    "        Parameters for scraping\n",
    "        :param base_url: The base url\n",
    "        :param test_mode: Test mode or not\n",
    "        :param p_area: Area of Japan\n",
    "        :param genre: Genre of food, ramen is the default\n",
    "        :param begin_page: Starting page, default is 1\n",
    "        :param end_page: Ending page, default is 10\n",
    "        \"\"\"\n",
    "        \n",
    "        # Store / restaurant info\n",
    "        self.store_id = \"\"\n",
    "        self.store_id_num = 0\n",
    "        self.store_name = \"\"\n",
    "        self.score = 0\n",
    "        self.p_area = p_area\n",
    "        self.genre = genre\n",
    "        \n",
    "        # Price range variables\n",
    "        self.daytime_price = \"\"\n",
    "        self.daytime_price_low = None\n",
    "        self.daytime_price_high = None\n",
    "        self.nighttime_price = \"\"\n",
    "        self.nighttime_price_low = None\n",
    "        self.nighttime_price_high = None\n",
    "        \n",
    "        # Engagement counts\n",
    "        self.review_count = 0\n",
    "        self.photo_count = 0\n",
    "        self.like_count = 0\n",
    "        self.bookmark_count = 0\n",
    "        self.columns = [\"store_id\", \"store_name\", \"score\", \"area\", \"genre\", \"review_count\", \n",
    "                        \"daytime_price\", \"daytime_price_low\", \"daytime_price_high\", \"nighttime_price\", \"nighttime_price_low\", \"nighttime_price_high\", \"photo_count\", \"like_count\", \"bookmark_count\"]\n",
    "        self.df = pd.DataFrame(columns=self.columns)\n",
    "        # \\n = newline\n",
    "        # \\s = whitespace\n",
    "        self.__regexcomp = re.compile(r\"\\n|\\s\")\n",
    "        \n",
    "        # Add a counter for successful stores\n",
    "        self.successful_stores = 0\n",
    "        # Loading bar progress\n",
    "        # bar = tqdm.tqdm(total=end_page)\n",
    "        \n",
    "        # Starting page number\n",
    "        page_num = begin_page\n",
    "        \n",
    "        if test_mode:\n",
    "            print(f\"Test Mode: Only processing up to 3 restaurants from page {begin_page}\")\n",
    "            # Sort by Tabelog rating score\n",
    "            list_url = base_url + str(page_num) + \"?select_sort_flg=1\" # \"/?Srt=D&SrtT=rt&sort_mode=1\" \n",
    "            self.scrape_list(list_url, mode=test_mode)\n",
    "            print(f\"Test complete - processed {self.successful_stores} restaurants\")\n",
    "        else:\n",
    "            print(f\"Starting full scrape from page {begin_page} to {end_page}\")\n",
    "            while True:\n",
    "                print(f\"Processing page {page_num}\")\n",
    "                # Sort by Tabelog rating score\n",
    "                list_url = base_url + str(page_num) + \"?select_sort_flg=1\" # \"/?Srt=D&SrtT=rt&sort_mode=1\" \n",
    "                if not self.scrape_list(list_url, mode=test_mode):\n",
    "                    print(f\"No more results found after page {page_num-1}\")\n",
    "                    break\n",
    "                \n",
    "                # Stop after reaching the end page\n",
    "                if page_num >= end_page:\n",
    "                    print(f\"Reached specified end page {end_page}\")\n",
    "                    break\n",
    "                page_num += 1\n",
    "                # bar.update(1)\n",
    "            \n",
    "            print(f\"Scraping complete, processed {self.successful_stores} stores / restaurants\")\n",
    "        return\n",
    "\n",
    "    def scrape_list(self, list_url, mode):\n",
    "        \"\"\"\n",
    "        Parse a whole restaurant list page\n",
    "        \"\"\"\n",
    "        with urllib.request.urlopen(list_url) as r:\n",
    "            content = r.read()\n",
    "            status_code = r.status\n",
    "        if status_code != http.client.OK:\n",
    "            return False\n",
    "        \n",
    "        # Put all content into soup parser\n",
    "        soup = BeautifulSoup(content, \"html.parser\")\n",
    "        # Restaurant name list\n",
    "        soup_a_list = soup.find_all(\"a\", class_=\"list-rst__rst-name-target\") \n",
    "\n",
    "        # If there's nothing, return\n",
    "        if len(soup_a_list) == 0:\n",
    "            return False\n",
    "\n",
    "        if mode:\n",
    "            # In test mode, only scrape the first 3 restaurants\n",
    "            for soup_a in soup_a_list[:3]:\n",
    "                 # Get individual restaurant page URL\n",
    "                item_url = soup_a.get(\"href\")\n",
    "                self.store_id_num += 1\n",
    "                self.scrape_item(item_url)\n",
    "        else:\n",
    "            # In normal mode, scrape all restaurants on the page (usually 20 entries)\n",
    "            for soup_a in soup_a_list:\n",
    "                # Get the individual restaurant page URL\n",
    "                item_url = soup_a.get(\"href\")\n",
    "                self.store_id_num += 1\n",
    "                self.scrape_item(item_url)\n",
    "\n",
    "        return True\n",
    "\n",
    "    def scrape_item(self, item_url):\n",
    "        \"\"\"\n",
    "        Parse an individual restaurant page\n",
    "        \"\"\"\n",
    "        # Request site content\n",
    "        with urllib.request.urlopen(item_url) as r:\n",
    "            content = r.read()\n",
    "            status_code = r.status\n",
    "        if status_code != http.client.OK:\n",
    "            print(f\"error: not found{ item_url }\")\n",
    "            return\n",
    "\n",
    "        # Add delay to avoid hitting rate limits\n",
    "        time.sleep(1)\n",
    "        \n",
    "        # Parse with soup\n",
    "        soup = BeautifulSoup(content, \"html.parser\")\n",
    "        \n",
    "        # Get restaurant name\n",
    "        store_name_tag = soup.find(\"h2\", class_=\"display-name\")\n",
    "        if not store_name_tag or not store_name_tag.span:\n",
    "            print(f\"error: cannot find restaurant name at {item_url}\")\n",
    "            self.store_id_num -= 1\n",
    "            return\n",
    "            \n",
    "        store_name = store_name_tag.span.string\n",
    "        print(\"{}→店名：{}\".format(self.store_id_num, store_name.strip()))\n",
    "        self.store_name = store_name.strip()\n",
    "        \n",
    "        # Store header info\n",
    "        store_head = soup.find(\"div\", class_=\"rdheader-subinfo\")\n",
    "        if not store_head:\n",
    "            print(\"Cannot find store header information, skipping\")\n",
    "            self.store_id_num -= 1\n",
    "            return\n",
    "            \n",
    "        store_head_list = store_head.find_all(\"dl\")\n",
    "        if len(store_head_list) < 2:\n",
    "            print(\"Store header information is incomplete, skipping\")\n",
    "            self.store_id_num -= 1\n",
    "            return\n",
    "            \n",
    "        store_head_list = store_head_list[1].find_all(\"span\")\n",
    "        if not store_head_list:\n",
    "            print(\"Cannot determine store type, skipping\")\n",
    "            self.store_id_num -= 1\n",
    "            return\n",
    "        \n",
    "        # Check if it's a ramen or tsukemen restaurant\n",
    "        # if store_head_list[0].text not in {\"ラーメン\", \"つけ麺\"}:\n",
    "        #     print(\"Not a ramen or tsukemen restaurant, skipping\")\n",
    "        #     self.store_id_num -= 1\n",
    "        #     return\n",
    "        \n",
    "        # Get rating score\n",
    "        rating_score_tag = soup.find(\"b\", class_=\"c-rating__val\")\n",
    "        if not rating_score_tag or not rating_score_tag.span:\n",
    "            print(\"評価が見つかりません\")\n",
    "            self.store_id_num -= 1\n",
    "            return\n",
    "            \n",
    "        rating_score = rating_score_tag.span.string\n",
    "        print(\"評価点数：{}点\".format(rating_score), end=\"\")\n",
    "        self.score = rating_score\n",
    "        \n",
    "        # Skip restaurants with no rating\n",
    "        if rating_score == \"-\":\n",
    "            print(\"  評価がないため処理対象外\")\n",
    "            self.store_id_num -= 1\n",
    "            return\n",
    "               \n",
    "        # This code skips restaurants with rating below 3.5\n",
    "        # Good for looking for good places\n",
    "        # if float(rating_score) < 3.5:\n",
    "        #     print(\"  食べログ評価が3.5未満のため処理対象外\")\n",
    "        #     self.store_id_num -= 1\n",
    "        #     return\n",
    "        \n",
    "        # Get review count\n",
    "        review_tag_id = soup.find(\"li\", id=\"rdnavi-review\")\n",
    "        if not review_tag_id or not review_tag_id.a:\n",
    "            print(\"  レビューセクションが見つかりません\")\n",
    "            self.store_id_num -= 1\n",
    "            return\n",
    "            \n",
    "        review_tag = review_tag_id.a.get(\"href\")\n",
    "        \n",
    "        # Get review count\n",
    "        review_count_span = review_tag_id.find(\"span\", class_=\"rstdtl-navi__total-count\")\n",
    "        if not review_count_span or not review_count_span.em:\n",
    "            print(\"  レビュー件数が見つかりません\")\n",
    "            self.review_count = 0\n",
    "        else:\n",
    "            print(\"  レビュー件数：{}\".format(review_count_span.em.string), end=\"\")\n",
    "            self.review_count = review_count_span.em.string\n",
    "        \n",
    "              # Get price information (daytime and nighttime)\n",
    "        # Find all price target elements\n",
    "        price_targets = soup.find_all(\"a\", class_=\"rdheader-budget__price-target\")\n",
    "        \n",
    "         # For each price target, determine if it's lunch or dinner\n",
    "        for price_target in price_targets:\n",
    "            # Find the closest time indicator (lunch or dinner icon)\n",
    "            parent_element = price_target.parent\n",
    "            while parent_element and not parent_element.find(\"i\", class_=\"c-rating-v3__time\"):\n",
    "                parent_element = parent_element.parent\n",
    "                \n",
    "            time_elem = parent_element.find(\"i\", class_=\"c-rating-v3__time\") if parent_element else None\n",
    "            \n",
    "            if time_elem and price_target:\n",
    "                price_text = price_target.text.strip()\n",
    "                \n",
    "                # Check class list for 'lunch' or 'dinner' substring\n",
    "                class_list = time_elem.get(\"class\", [])\n",
    "                is_lunch = any(\"lunch\" in class_name for class_name in class_list)\n",
    "                is_dinner = any(\"dinner\" in class_name for class_name in class_list)\n",
    "                \n",
    "                # Assign price to appropriate category\n",
    "                if is_lunch:\n",
    "                    self.daytime_price, self.daytime_price_low, self.daytime_price_high = parse_price_range(price_text)\n",
    "                elif is_dinner:\n",
    "                    self.nighttime_price, self.nighttime_price_low, self.nighttime_price_high = parse_price_range(price_text)  \n",
    "            \n",
    "        # Display the extracted price information\n",
    "        day_info = f\"{self.daytime_price} ({self.daytime_price_low}-{self.daytime_price_high})\" if self.daytime_price else \"-\"\n",
    "        night_info = f\"{self.nighttime_price} ({self.nighttime_price_low}-{self.nighttime_price_high})\" if self.nighttime_price else \"-\"\n",
    "        print(f\"  価格帯：昼 {day_info}、夜 {night_info}\", end=\"\")\n",
    "        \n",
    "        # Get the review list page URL and scrape the first page of reviews\n",
    "        page_num = 1  # Just scrape the first page of reviews\n",
    "        \n",
    "        if review_tag:\n",
    "            review_url = review_tag + \"COND-0/smp1/?lc=0&rvw_part=all&PG=\" + str(page_num)\n",
    "            # Used after other information is obtained\n",
    "            print(\" . \", end=\"\")\n",
    "            self.scrape_review_page(review_url)\n",
    "    \n",
    "        self.make_df()\n",
    "        return\n",
    "\n",
    "    def scrape_review_page(self, review_url):\n",
    "        \"\"\"\n",
    "        Parse the review list page and extract metrics\n",
    "        \"\"\"\n",
    "        # Add a small delay before requesting review page\n",
    "        time.sleep(0.5)\n",
    "        \n",
    "        with urllib.request.urlopen(review_url) as r:\n",
    "            content = r.read()\n",
    "            status_code = r.status\n",
    "        if status_code != http.client.OK:\n",
    "            print(f\"error: not found{ review_url }\")\n",
    "            return False\n",
    "\n",
    "        soup = BeautifulSoup(content, \"html.parser\")\n",
    "        review_items = soup.find_all(\"div\", class_=\"rvw-item\")\n",
    "        \n",
    "        if len(review_items) == 0:\n",
    "            return False\n",
    "        \n",
    "        total_photos = 0\n",
    "        total_likes = 0\n",
    "        total_bookmarks = 0\n",
    "        \n",
    "        for review_item in review_items:\n",
    "            # Count photos\n",
    "            photo_section = review_item.find(\"div\", class_=\"rvw-photo\")\n",
    "            if photo_section:\n",
    "                # Check for \"more photos\" indicator\n",
    "                more_photos = photo_section.find(\"span\", class_=\"c-photo-more__num\")\n",
    "                if more_photos:\n",
    "                    try:\n",
    "                        total_photos += int(more_photos.text)\n",
    "                    except ValueError:\n",
    "                        pass\n",
    "                \n",
    "                # Count visible photos\n",
    "                photo_list = photo_section.find_all(\"li\", class_=\"rvw-photo__list-item\")\n",
    "                if photo_list:\n",
    "                    total_photos += len(photo_list)\n",
    "            \n",
    "            # Count likes\n",
    "            like_count_elem = review_item.find(\"div\", class_=\"js-like-source\")\n",
    "            if like_count_elem:\n",
    "                try:\n",
    "                    like_data = re.search(r'\"count\":(\\d+)', like_count_elem.text)\n",
    "                    if like_data:\n",
    "                        total_likes += int(like_data.group(1))\n",
    "                except (ValueError, AttributeError):\n",
    "                    pass\n",
    "            \n",
    "            # Count bookmarks\n",
    "            bookmark_elem = review_item.find(\"div\", class_=\"js-vote-interest\")\n",
    "            if bookmark_elem and bookmark_elem.get(\"data-hozon-count\"):\n",
    "                try:\n",
    "                    total_bookmarks += int(bookmark_elem.get(\"data-hozon-count\"))\n",
    "                except ValueError:\n",
    "                    pass\n",
    "            \n",
    "        # Update counters\n",
    "        self.photo_count = total_photos\n",
    "        self.like_count = total_likes\n",
    "        self.bookmark_count = total_bookmarks\n",
    "        \n",
    "        print(f\"  写真：{total_photos}枚、いいね：{total_likes}件、保存：{total_bookmarks}件\")\n",
    "        return True\n",
    "    \n",
    "    def make_df(self):\n",
    "        \"\"\"\n",
    "        Create a DataFrame row with the collected data\n",
    "        \"\"\"\n",
    "        # Use 0-padding\n",
    "        self.store_id = str(self.store_id_num).zfill(8) \n",
    "        # Create row for the DataFrame\n",
    "        se = pd.Series([\n",
    "            self.store_id, \n",
    "            self.store_name, \n",
    "            self.score, \n",
    "            self.p_area, \n",
    "            self.genre,\n",
    "            self.review_count, \n",
    "            self.daytime_price,\n",
    "            self.daytime_price_low,\n",
    "            self.daytime_price_high,\n",
    "            self.nighttime_price,\n",
    "            self.nighttime_price_low,\n",
    "            self.nighttime_price_high,\n",
    "            self.photo_count, \n",
    "            self.like_count, \n",
    "            self.bookmark_count, \n",
    "        ], self.columns) \n",
    "        # Add the row to the DF\n",
    "        self.df = pd.concat([self.df, pd.DataFrame([se], columns=self.columns)], ignore_index=True)\n",
    "        # Increment the successful stores counter\n",
    "        self.successful_stores += 1\n",
    "        \n",
    "        # Reset values for next restaurant\n",
    "        self.photo_count = 0\n",
    "        self.like_count = 0\n",
    "        self.bookmark_count = 0\n",
    "        self.daytime_price = \"\"\n",
    "        self.daytime_price_low = None\n",
    "        self.daytime_price_high = None\n",
    "        self.nighttime_price = \"\"\n",
    "        self.nighttime_price_low = None\n",
    "        self.nighttime_price_high = None\n",
    "        return\n",
    "    \n",
    "def parse_price_range(price_text):\n",
    "    \"\"\"\n",
    "    Parse price ranges like \"￥1,000～￥1,999\" into low and high values\n",
    "    Returns a tuple of (original_text, low_value, high_value)\n",
    "    \"\"\"\n",
    "    if not price_text or price_text == \"-\":\n",
    "        return price_text, None, None\n",
    "        \n",
    "    # Regex the numbers from the price range\n",
    "    numbers = re.findall(r\"￥([0-9,]+)\", price_text)\n",
    "    \n",
    "    if len(numbers) >= 2:\n",
    "        # Convert to integers and remove commas\n",
    "        try:\n",
    "            low = int(numbers[0].replace(\",\", \"\"))\n",
    "            high = int(numbers[1].replace(\",\", \"\"))\n",
    "            return price_text, low, high\n",
    "        except ValueError:\n",
    "            pass\n",
    "    elif len(numbers) >= 1 and price_text.startswith(\"～￥\"):\n",
    "        # If price range doesn't start with a number, make lower bound 1 and use upper bound normally\n",
    "        low = 1\n",
    "        high = numbers[0]\n",
    "        return price_text, low, high\n",
    "    \n",
    "    return price_text, None, None"
   ],
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-04-11T20:18:20.936088Z",
     "start_time": "2025-04-11T20:18:20.890563Z"
    }
   },
   "id": "initial_id",
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Food categories: 15, Total URLs to process: 15\n"
     ]
    }
   ],
   "source": [
    "# Base URL for Tokyo restaurants\n",
    "tokyo_base_url = \"https://tabelog.com/tokyo/rstLst/\"\n",
    "# Categories of food\n",
    "food_categories = [\n",
    "    \"ramen\", \n",
    "    \"japanese\", \n",
    "    \"washoku\", \n",
    "    \"sushi\", \n",
    "    \"seafood\", \n",
    "    \"udon\", \n",
    "    \"yakiniku\", \n",
    "    \"curry\", \n",
    "    \"italian\", \n",
    "    \"izakaya\", \n",
    "    \"sweets\", \n",
    "    \"chinese\", \n",
    "    \"pizza\", \n",
    "    \"syabusyabu\", \n",
    "    \"korea\"\n",
    "]\n",
    "food_category_urls_tokyo = []\n",
    "for category in food_categories:\n",
    "    food_category_urls_tokyo.append(tokyo_base_url + category + \"/\")\n",
    "\n",
    "print(f\"Food categories: {len(food_categories)}, Total URLs to process: {len(food_category_urls_tokyo)}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-04-11T20:18:43.770846Z",
     "start_time": "2025-04-11T20:18:43.766001Z"
    }
   },
   "id": "531bbb1fe407e97",
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Progress bar\n",
    "progress = tqdm.tqdm(total=len(food_category_urls_tokyo))\n",
    "for category, tokyo_food_category_url in zip(food_categories, food_category_urls_tokyo):\n",
    "    print(f\"Processing category - {category}\")\n",
    "    output_csv_name = \"./data/food_category_data/tabelog_tokyo_\" + category + \"_data.csv\"\n",
    "    # Tokyo Ramen Rating Data, 20 entries per page - 10 pages → 200 entries \n",
    "    tabelog_tokyo_genre = Tabelog(tokyo_base_url, test_mode=False, p_area=\"東京都内\", genre=category, begin_page=1, end_page=10)\n",
    "    tabelog_tokyo_genre.df.to_csv(output_csv_name, encoding=\"utf-8-sig\", index=False)\n",
    "    progress.update(1)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "71c3c7c559785452",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Use this if prices are missing for rows that start with \"～￥\" or have \"～￥999\" imputed\n",
    "for category in food_categories:\n",
    "    output_csv_name = \"./data/food_category_data/tabelog_tokyo_\" + category + \"_data.csv\"\n",
    "    tabelog_tokyo_df = pd.read_csv(output_csv_name, encoding=\"utf-8-sig\")\n",
    "    tblg_df_copy = tabelog_tokyo_df.copy()\n",
    "    \n",
    "    # Find rows where price is \"～￥999\"\n",
    "    daytime_mask = tblg_df_copy[\"daytime_price\"] == \"～￥999\"\n",
    "    nighttime_mask = tblg_df_copy[\"nighttime_price\"] == \"～￥999\"\n",
    "    \n",
    "    # Impute missing values\n",
    "    tblg_df_copy.loc[daytime_mask, \"daytime_price_low\"] = 1\n",
    "    tblg_df_copy.loc[daytime_mask, \"daytime_price_high\"] = 999\n",
    "    tblg_df_copy.loc[nighttime_mask, \"nighttime_price_low\"] = 1\n",
    "    tblg_df_copy.loc[nighttime_mask, \"nighttime_price_high\"] = 999\n",
    "    \n",
    "    # Write CSV\n",
    "    tblg_df_copy.to_csv(output_csv_name, encoding=\"utf-8-sig\", index=False)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-04-11T20:20:29.928372Z",
     "start_time": "2025-04-11T20:20:29.867219Z"
    }
   },
   "id": "db337e71e41d7a9",
   "execution_count": 9
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# for category in food_categories:\n",
    "#     output_csv_name = \"./data/food_category_data/tabelog_tokyo_\" + category + \"_data.csv\"\n",
    "#     tabelog_tokyo_df = pd.read_csv(output_csv_name, encoding=\"utf-8-sig\")\n",
    "#     if \"Unnamed: 0.1\" in tabelog_tokyo_df.columns:\n",
    "#         print(\"Removing unnamed columns\")\n",
    "#         tabelog_tokyo_df = tabelog_tokyo_df.drop(\"Unnamed: 0.1\", axis=1)\n",
    "#         tabelog_tokyo_df.to_csv(output_csv_name, encoding=\"utf-8-sig\", index=False)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "38c17c67ed674cac",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Insert genre column for all categories\n",
    "# for category in food_categories:\n",
    "#     output_csv_name = \"./data/food_category_data/tabelog_tokyo_\" + category + \"_data.csv\"\n",
    "#     tabelog_tokyo_df = pd.read_csv(output_csv_name, encoding=\"utf-8-sig\")\n",
    "#     tblg_df_copy = tabelog_tokyo_df.copy()\n",
    "#     tblg_df_copy.insert(loc=4, column=\"genre\", value=category)\n",
    "#     tblg_df_copy.to_csv(output_csv_name, encoding=\"utf-8-sig\", index=False)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-04-11T20:50:25.790274Z",
     "start_time": "2025-04-11T20:50:25.719288Z"
    }
   },
   "id": "f6f29ee2e58cd34",
   "execution_count": 45
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import glob\n",
    "import os"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-04-11T20:58:03.692802Z",
     "start_time": "2025-04-11T20:58:03.683962Z"
    }
   },
   "id": "acbaaea441aa4094",
   "execution_count": 46
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Combine all CSV data into one dataset\n",
    "# This is probably not needed\n",
    "def combine_tabelog_csvs(directory_path):\n",
    "    # Get all CSV files in the directory\n",
    "    csv_files = glob.glob(os.path.join(directory_path, \"*.csv\"))\n",
    "    # Combined DataFrame\n",
    "    combined_tl_df = pd.DataFrame()\n",
    "    \n",
    "    # Process each CSV file\n",
    "    for f in csv_files:\n",
    "        # Read the current CSV\n",
    "        df = pd.read_csv(f)\n",
    "        \n",
    "        # Check if columns exist\n",
    "        expected_columns = [\"store_id\", \"store_name\", \"score\", \"area\", \"genre\", \"review_count\",\n",
    "                           \"daytime_price\", \"daytime_price_low\", \"daytime_price_high\",\n",
    "                           \"nighttime_price\", \"nighttime_price_low\", \"nighttime_price_high\",\n",
    "                           \"photo_count\", \"like_count\", \"bookmark_count\"]\n",
    "        missing_cols = [col for col in expected_columns if col not in df.columns]\n",
    "        if len(missing_cols) > 0:\n",
    "            continue\n",
    "            \n",
    "        # Append to the combined dataframe\n",
    "        combined_tl_df = pd.concat([combined_tl_df, df], ignore_index=True)\n",
    "    \n",
    "    # Randomize rows\n",
    "    combined_tl_df = combined_tl_df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "    # Reset store_id to be sequential from 1 to N\n",
    "    combined_tl_df[\"store_id\"] = range(1, len(combined_tl_df) + 1)\n",
    "    \n",
    "    return combined_tl_df\n",
    "\n",
    "\n",
    "combined_datasets = combine_tabelog_csvs(\"./data/food_category_data\")\n",
    "combined_datasets.to_csv(\"./data/tabelog_tokyo_data.csv\", encoding=\"utf-8-sig\", index=False)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-04-11T21:03:24.131367Z",
     "start_time": "2025-04-11T21:03:24.072148Z"
    }
   },
   "id": "8c491264c5a5526a",
   "execution_count": 50
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Tabelog Review Data"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c70dbd30d7c120cf"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def print_debug(*args, sep=\" \", end=\"\\n\", file=None, flush=False, debug=False):\n",
    "    if debug:\n",
    "        print(*args, sep=sep, end=end, file=file, flush=flush)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-04-13T01:08:35.383872Z",
     "start_time": "2025-04-13T01:08:35.378936Z"
    }
   },
   "id": "d4bc8f8c0f94441e",
   "execution_count": 36
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "class TabelogReviewScraper:\n",
    "    \"\"\"\n",
    "    Tabelog scraper that focuses on collecting reviews/ratings from restaurants\n",
    "    \"\"\"\n",
    "    def __init__(self, restaurant_url, max_pages=None, debug=False):\n",
    "        \"\"\"\n",
    "        Initialize scraper for a specific restaurant\n",
    "        :param restaurant_url: URL of the restaurant page\n",
    "        :param max_pages: Maximum number of review pages to scrape (default: None = all pages)\n",
    "        \"\"\"\n",
    "        # Info about accessing the restaurant / store data\n",
    "        self.restaurant_url = restaurant_url\n",
    "        self.max_pages = max_pages\n",
    "        \n",
    "        # Restaurant info\n",
    "        self.store_id = \"\"\n",
    "        self.store_name = \"\"\n",
    "        self.review_count = 0\n",
    "        \n",
    "        # User count\n",
    "        self.user_count = 0\n",
    "        \n",
    "        # For debugging\n",
    "        self.debug = debug\n",
    "        self.page_num = 0\n",
    "        \n",
    "        # DataFrame for reviews with sub-ratings\n",
    "        self.columns = [\n",
    "            \"user_id\", \"overall_rating\", \"food\", \"service\", \"atmosphere\", \"price\", \"drink\"\n",
    "        ]\n",
    "        self.reviews_df = pd.DataFrame(columns=self.columns)\n",
    "        \n",
    "        print_debug(f\"Starting scrape of restaurant reviews at {restaurant_url}\", debug=self.debug)\n",
    "        self.scrape_restaurant()\n",
    "        print_debug(f\"Scraping complete. Collected {len(self.reviews_df)} reviews\", debug=self.debug)\n",
    "        \n",
    "    def scrape_restaurant(self):\n",
    "        \"\"\"\n",
    "        Scrape the main restaurant page to get name and review link\n",
    "        \"\"\"\n",
    "        try:\n",
    "            with urllib.request.urlopen(self.restaurant_url) as r:\n",
    "                content = r.read()\n",
    "                status_code = r.status\n",
    "                \n",
    "            if status_code != http.client.OK:\n",
    "                print_debug(f\"Error: Could not access {self.restaurant_url}\", debug=self.debug)\n",
    "                return\n",
    "                \n",
    "            # Parse with soup\n",
    "            soup = BeautifulSoup(content, \"html.parser\")\n",
    "            \n",
    "            # Extract store ID from URL\n",
    "            self.store_id = self.restaurant_url.split(\"/\")[-2]\n",
    "            \n",
    "            # Get restaurant name\n",
    "            store_name_tag = soup.find(\"h2\", class_=\"display-name\")\n",
    "            if not store_name_tag or not store_name_tag.span:\n",
    "                print_debug(f\"Error: Cannot find restaurant name at {self.restaurant_url}\", debug=self.debug)\n",
    "                # Fallback to ID if name not found\n",
    "                self.store_name = self.store_id\n",
    "                return\n",
    "                \n",
    "            self.store_name = store_name_tag.span.string.strip()\n",
    "            print_debug(f\"Restaurant: {self.store_name} (ID: {self.store_id})\", debug=self.debug)\n",
    "            \n",
    "            # Get review link and count\n",
    "            review_tag_id = soup.find(\"li\", id=\"rdnavi-review\")\n",
    "            if not review_tag_id or not review_tag_id.a:\n",
    "                print_debug(\"ロコミのページが見つかりません\", debug=self.debug)\n",
    "                return\n",
    "                \n",
    "            review_tag = review_tag_id.a.get(\"href\")\n",
    "            \n",
    "            # Get review count\n",
    "            self.review_count = 0\n",
    "            review_count_span = review_tag_id.find(\"span\", class_=\"rstdtl-navi__total-count\")\n",
    "            if review_count_span and review_count_span.em:\n",
    "                self.review_count = int(review_count_span.em.string)\n",
    "                print_debug(f\"Total reviews: {self.review_count}\", debug=self.debug)\n",
    "            else:\n",
    "                print_debug(\"Review count not found, will scrape\", debug=self.debug)\n",
    "            \n",
    "            # Calculate pages based on review count (20 reviews per page)\n",
    "            if self.review_count > 0:\n",
    "                estimated_pages = -((self.review_count + 19) // -20)\n",
    "                print_debug(f\"Estimated number of pages for {self.store_name}: {estimated_pages - 1}〜{estimated_pages}\", debug=self.debug)\n",
    "            \n",
    "            # Start scraping review pages\n",
    "            self.page_num = 1\n",
    "            last_page = False\n",
    "            \n",
    "            while not last_page:\n",
    "                # Check if the maximum pages limit is reached\n",
    "                if self.max_pages and self.page_num > self.max_pages:\n",
    "                    print_debug(f\"Reached maximum page limit ({self.max_pages})\", debug=self.debug)\n",
    "                    break\n",
    "                \n",
    "                print_debug(f\"ロコミ{self.page_num}ページ目が加工されています\", debug=self.debug)\n",
    "                review_url = review_tag + f\"COND-0/smp1/?lc=0&rvw_part=all&PG={self.page_num}\"\n",
    "                \n",
    "                # Scrape the current page\n",
    "                result = self.scrape_review_page(review_url)\n",
    "                \n",
    "                if not result:\n",
    "                    print_debug(f\"Error accessing page {self.page_num} or no reviews found\", debug=self.debug)\n",
    "                    break\n",
    "                \n",
    "                soup, reviews_found = result\n",
    "                \n",
    "                if not reviews_found:\n",
    "                    print_debug(f\"No reviews found on page {self.page_num}\", debug=self.debug)\n",
    "                    break\n",
    "                \n",
    "                # Check for pagination to determine if there are more pages\n",
    "                pagination = soup.find(\"div\", class_=\"c-pagination\")\n",
    "                \n",
    "                # If there's no pagination element at all, and we're on page 1, \n",
    "                # this means there's only one page of reviews\n",
    "                if not pagination and self.page_num == 1:\n",
    "                    print_debug(\"Only one page of reviews found\", debug=self.debug)\n",
    "                    last_page = True\n",
    "                elif pagination:\n",
    "                    # Look for the \"next\" button\n",
    "                    next_button = pagination.find(\"a\", class_=\"c-pagination__arrow--next\")\n",
    "                    \n",
    "                    # If there's no next button, or if the current page is the last one,\n",
    "                    # we've reached the end\n",
    "                    if not next_button:\n",
    "                        print_debug(f\"Reached the last page of reviews ({self.page_num})\", debug=self.debug)\n",
    "                        last_page = True\n",
    "                else:\n",
    "                    # No pagination found after page 1, must be the last page\n",
    "                    print_debug(f\"No more pages found after page {self.page_num}\", debug=self.debug)\n",
    "                    last_page = True\n",
    "                \n",
    "                # Move to the next page\n",
    "                if not last_page:\n",
    "                    self.page_num += 1\n",
    "                    # Pause between pages to avoid rate limiting\n",
    "                    time.sleep(2)\n",
    "                \n",
    "        except Exception as e:\n",
    "            print_debug(f\"Error scraping restaurant: {e}\", debug=self.debug)\n",
    "            \n",
    "    def scrape_review_page(self, review_url):\n",
    "        \"\"\"\n",
    "        Scrape a single page of reviews to extract ratings\n",
    "        Returns tuple of (soup, reviews_found) or None if error\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Add delay to prevent ratelimits\n",
    "            time.sleep(1)\n",
    "            \n",
    "            # Request\n",
    "            with urllib.request.urlopen(review_url) as r:\n",
    "                content = r.read()\n",
    "                status_code = r.status\n",
    "                \n",
    "            if status_code != http.client.OK:\n",
    "                print_debug(f\"Error: Could not access {review_url}\", debug=self.debug)\n",
    "                return None\n",
    "    \n",
    "            # Access contents of page\n",
    "            soup = BeautifulSoup(content, \"html.parser\")\n",
    "            review_items = soup.find_all(\"div\", class_=\"rvw-item\")\n",
    "            \n",
    "            # If there are no items to review\n",
    "            if len(review_items) == 0:\n",
    "                print_debug(\"ロコミが見つかりません\", debug=self.debug)\n",
    "                return soup, False\n",
    "            \n",
    "            # Number of reviews in the page\n",
    "            print_debug(f\"{self.page_num}ページ目にレビュー{len(review_items)}件\", debug=self.debug)\n",
    "            \n",
    "            # Parse all ratings and add them to DF\n",
    "            for review_item in review_items:\n",
    "                self.parse_review_ratings(review_item)\n",
    "                \n",
    "            return soup, True\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error scraping review page: {e}\")\n",
    "            return None\n",
    "            \n",
    "    def parse_review_ratings(self, review_item):\n",
    "        \"\"\"\n",
    "        Parse only the ratings from an individual review\n",
    "        Uses sequential user IDs\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Initialize ratings\n",
    "            # None (will be converted to NaN in DataFrame)\n",
    "            overall_rating = None\n",
    "            food_rating = None\n",
    "            service_rating = None\n",
    "            # The atmosphere of the place \n",
    "            atmosphere_rating = None\n",
    "            # CP (Cost Performance)\n",
    "            price_rating = None\n",
    "            # Rating of the drinks\n",
    "            drink_rating = None\n",
    "            \n",
    "            # Increment user ID\n",
    "            self.user_count += 1\n",
    "            \n",
    "            # Get overall rating\n",
    "            rating_elem = review_item.find([\"p\", \"div\"], class_=\"c-rating-v3--xl\")\n",
    "            if rating_elem:\n",
    "                val_elem = rating_elem.find(\"b\", class_=\"c-rating-v3__val\")\n",
    "                if val_elem:\n",
    "                    try:\n",
    "                        overall_rating = float(val_elem.text.strip())\n",
    "                    except ValueError:\n",
    "                        pass\n",
    "            \n",
    "            # Get detailed ratings\n",
    "            rating_detail = review_item.find(\"ul\", class_=\"c-rating-detail\")\n",
    "            if rating_detail:\n",
    "                rating_items = rating_detail.find_all(\"li\", class_=\"c-rating-detail__item\")\n",
    "                for item in rating_items:\n",
    "                    label = item.find(\"span\")\n",
    "                    value = item.find(\"strong\")\n",
    "                    if label and value:\n",
    "                        # Get label and rating values\n",
    "                        label_text = label.text.strip()\n",
    "                        value_text = value.text.strip()\n",
    "                        \n",
    "                        try:\n",
    "                            rating_value = float(value_text)\n",
    "                            # Extract all ratings\n",
    "                            if \"料理・味\" in label_text:\n",
    "                                food_rating = rating_value\n",
    "                            elif \"サービス\" in label_text:\n",
    "                                service_rating = rating_value\n",
    "                            elif \"雰囲気\" in label_text:\n",
    "                                atmosphere_rating = rating_value\n",
    "                            elif \"CP\" in label_text:\n",
    "                                price_rating = rating_value\n",
    "                            elif \"酒・ドリンク\" in label_text:\n",
    "                                drink_rating = rating_value\n",
    "                        except ValueError:\n",
    "                            # Skip if conversion to float somehow fails\n",
    "                            pass\n",
    "            \n",
    "            # Add to DataFrame and only add if at least one rating is present\n",
    "            ratings = [overall_rating, food_rating, service_rating, atmosphere_rating, price_rating, drink_rating]\n",
    "            if any(type(r) == float for r in ratings):\n",
    "                # Series with user id of 6 digits\n",
    "                se = pd.Series([\n",
    "                    str(self.user_count).zfill(6),\n",
    "                    *ratings\n",
    "                ], self.columns) \n",
    "                r_df = pd.DataFrame([se], columns=self.columns)\n",
    "                for col in self.columns:\n",
    "                    if col in self.reviews_df.columns:\n",
    "                        r_df[col] = r_df[col].astype(self.reviews_df[col].dtype)\n",
    "                # Add the row to the DF\n",
    "                self.reviews_df = pd.concat([self.reviews_df, r_df], ignore_index=True)\n",
    "        except Exception as e:\n",
    "            print(f\"Error parsing review: {e}\")\n",
    "            \n",
    "    def save_data(self, directory=\".\"):\n",
    "        \"\"\"\n",
    "        Save the ratings data to a CSV file\n",
    "        \"\"\"\n",
    "        # Remove problematic characters\n",
    "        store_name = re.sub(r'[\\\\/*?:\"<>|.]', \"\", self.store_name)\n",
    "        store_name = store_name.replace(\" \", \"_\")\n",
    "        \n",
    "        filename = os.path.join(directory, f\"tabelog_{store_name}_review_data.csv\")\n",
    "        self.reviews_df.to_csv(filename, encoding=\"utf-8-sig\", index=False)\n",
    "        print_debug(f\"Review ratings for {self.store_name} saved to {filename}\", debug=self.debug)\n",
    "        print_debug(f\"Collected {len(self.reviews_df)} reviews with {self.user_count} unique users\", debug=self.debug)\n",
    "        return filename"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-04-13T01:13:16.000088Z",
     "start_time": "2025-04-13T01:13:15.973778Z"
    }
   },
   "id": "e0212543752766b5",
   "execution_count": 42
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting scrape of restaurant reviews at https://tabelog.com/kanagawa/A1404/A140402/14032275/\n",
      "Restaurant: 鎌倉のごはんやさん 石渡 (ID: 14032275)\n",
      "Total reviews: 64\n",
      "Estimated number of pages for 鎌倉のごはんやさん 石渡: 5\n",
      "ロコミ1ページ目が加工されています\n",
      "1ページ目にレビュー21件\n",
      "ロコミ2ページ目が加工されています\n",
      "2ページ目にレビュー20件\n",
      "ロコミ3ページ目が加工されています\n",
      "3ページ目にレビュー20件\n",
      "ロコミ4ページ目が加工されています\n",
      "4ページ目にレビュー3件\n",
      "Reached the last page of reviews (4)\n",
      "Scraping complete. Collected 60 reviews\n",
      "Review ratings for 鎌倉のごはんやさん 石渡 saved to ./tabelog_review_data/tabelog_鎌倉のごはんやさん_石渡_review_data.csv\n",
      "Collected 60 reviews with 64 unique users\n"
     ]
    },
    {
     "data": {
      "text/plain": "'./tabelog_review_data/tabelog_鎌倉のごはんやさん_石渡_review_data.csv'"
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_restaurant_url = \"https://tabelog.com/kanagawa/A1404/A140402/14032275/\"\n",
    "scraper = TabelogReviewScraper(test_restaurant_url, max_pages=None, debug=True)\n",
    "scraper.save_data(directory=\"./tabelog_review_data\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-04-13T01:09:53.719525Z",
     "start_time": "2025-04-13T01:09:33.401453Z"
    }
   },
   "id": "80b383f5fd0b3057",
   "execution_count": 40
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "a9b7ca4f0428dba"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
