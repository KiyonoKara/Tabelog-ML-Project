{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Tabelog Data Collection"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e5479c65352a1ab"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "import http.client\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import pandas as pd\n",
    "import tqdm\n",
    "import time"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-04-11T20:18:19.502187Z",
     "start_time": "2025-04-11T20:18:19.083854Z"
    }
   },
   "id": "626395d738d7482c",
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "class Tabelog:\n",
    "    \"\"\"\n",
    "    Tabelog scraper for score, review count, daytime and nighttime price, photo count, like count, and bookmark count.\n",
    "    Credit to https://qiita.com/toshiyuki_tsutsui/items/f143946944a428ed105b for setting the foundation of this scraper. ありがとうございます。ദ്ദി(｡•̀ ,<)~✩‧₊\n",
    "    \"\"\"\n",
    "    def __init__(self, base_url, test_mode=False, p_area='東京都内', genre='ramen', begin_page=1, end_page=10):\n",
    "        \"\"\"\n",
    "        Parameters for scraping\n",
    "        :param base_url: The base url\n",
    "        :param test_mode: Test mode or not\n",
    "        :param p_area: Area of Japan\n",
    "        :param genre: Genre of food, ramen is the default\n",
    "        :param begin_page: Starting page, default is 1\n",
    "        :param end_page: Ending page, default is 10\n",
    "        \"\"\"\n",
    "        \n",
    "        # Store / restaurant info\n",
    "        self.store_id = ''\n",
    "        self.store_id_num = 0\n",
    "        self.store_name = ''\n",
    "        self.score = 0\n",
    "        self.p_area = p_area\n",
    "        self.genre = genre\n",
    "        \n",
    "        # Price range variables\n",
    "        self.daytime_price = ''\n",
    "        self.daytime_price_low = None\n",
    "        self.daytime_price_high = None\n",
    "        self.nighttime_price = ''\n",
    "        self.nighttime_price_low = None\n",
    "        self.nighttime_price_high = None\n",
    "        \n",
    "        # Engagement counts\n",
    "        self.review_count = 0\n",
    "        self.photo_count = 0\n",
    "        self.like_count = 0\n",
    "        self.bookmark_count = 0\n",
    "        self.columns = ['store_id', 'store_name', 'score', 'area', 'genre', 'review_count', \n",
    "                        'daytime_price', 'daytime_price_low', 'daytime_price_high', 'nighttime_price', 'nighttime_price_low', 'nighttime_price_high', 'photo_count', 'like_count', 'bookmark_count']\n",
    "        self.df = pd.DataFrame(columns=self.columns)\n",
    "        # \\n = newline\n",
    "        # \\s = whitespace\n",
    "        self.__regexcomp = re.compile(r'\\n|\\s')\n",
    "        \n",
    "        # Add a counter for successful stores\n",
    "        self.successful_stores = 0\n",
    "        # Loading bar progress\n",
    "        # bar = tqdm.tqdm(total=end_page)\n",
    "        \n",
    "        # Starting page number\n",
    "        page_num = begin_page\n",
    "        \n",
    "        if test_mode:\n",
    "            print(f\"Test Mode: Only processing up to 3 restaurants from page {begin_page}\")\n",
    "            # Sort by Tabelog rating score\n",
    "            list_url = base_url + str(page_num) + '?select_sort_flg=1' # '/?Srt=D&SrtT=rt&sort_mode=1' \n",
    "            self.scrape_list(list_url, mode=test_mode)\n",
    "            print(f\"Test complete - processed {self.successful_stores} restaurants\")\n",
    "        else:\n",
    "            print(f\"Starting full scrape from page {begin_page} to {end_page}\")\n",
    "            while True:\n",
    "                print(f\"Processing page {page_num}\")\n",
    "                # Sort by Tabelog rating score\n",
    "                list_url = base_url + str(page_num) + '?select_sort_flg=1' # '/?Srt=D&SrtT=rt&sort_mode=1' \n",
    "                if not self.scrape_list(list_url, mode=test_mode):\n",
    "                    print(f\"No more results found after page {page_num-1}\")\n",
    "                    break\n",
    "                \n",
    "                # Stop after reaching the end page\n",
    "                if page_num >= end_page:\n",
    "                    print(f\"Reached specified end page {end_page}\")\n",
    "                    break\n",
    "                page_num += 1\n",
    "                # bar.update(1)\n",
    "            \n",
    "            print(f\"Scraping complete, processed {self.successful_stores} stores / restaurants\")\n",
    "        return\n",
    "\n",
    "    def scrape_list(self, list_url, mode):\n",
    "        \"\"\"\n",
    "        Parse a whole restaurant list page\n",
    "        \"\"\"\n",
    "        with urllib.request.urlopen(list_url) as r:\n",
    "            content = r.read()\n",
    "            status_code = r.status\n",
    "        if status_code != http.client.OK:\n",
    "            return False\n",
    "        \n",
    "        # Put all content into soup parser\n",
    "        soup = BeautifulSoup(content, 'html.parser')\n",
    "        # Restaurant name list\n",
    "        soup_a_list = soup.find_all('a', class_='list-rst__rst-name-target') \n",
    "\n",
    "        # If there's nothing, return\n",
    "        if len(soup_a_list) == 0:\n",
    "            return False\n",
    "\n",
    "        if mode:\n",
    "            # In test mode, only scrape the first 3 restaurants\n",
    "            for soup_a in soup_a_list[:3]:\n",
    "                 # Get individual restaurant page URL\n",
    "                item_url = soup_a.get('href')\n",
    "                self.store_id_num += 1\n",
    "                self.scrape_item(item_url)\n",
    "        else:\n",
    "            # In normal mode, scrape all restaurants on the page (usually 20 entries)\n",
    "            for soup_a in soup_a_list:\n",
    "                # Get the individual restaurant page URL\n",
    "                item_url = soup_a.get('href')\n",
    "                self.store_id_num += 1\n",
    "                self.scrape_item(item_url)\n",
    "\n",
    "        return True\n",
    "\n",
    "    def scrape_item(self, item_url):\n",
    "        \"\"\"\n",
    "        Parse an individual restaurant page\n",
    "        \"\"\"\n",
    "        # Request site content\n",
    "        with urllib.request.urlopen(item_url) as r:\n",
    "            content = r.read()\n",
    "            status_code = r.status\n",
    "        if status_code != http.client.OK:\n",
    "            print(f'error: not found{ item_url }')\n",
    "            return\n",
    "\n",
    "        # Add delay to avoid hitting rate limits\n",
    "        time.sleep(1)\n",
    "        \n",
    "        # Parse with soup\n",
    "        soup = BeautifulSoup(content, 'html.parser')\n",
    "        \n",
    "        # Get restaurant name\n",
    "        store_name_tag = soup.find('h2', class_='display-name')\n",
    "        if not store_name_tag or not store_name_tag.span:\n",
    "            print(f'error: cannot find restaurant name at {item_url}')\n",
    "            self.store_id_num -= 1\n",
    "            return\n",
    "            \n",
    "        store_name = store_name_tag.span.string\n",
    "        print('{}→店名：{}'.format(self.store_id_num, store_name.strip()))\n",
    "        self.store_name = store_name.strip()\n",
    "        \n",
    "        # Store header info\n",
    "        store_head = soup.find('div', class_='rdheader-subinfo')\n",
    "        if not store_head:\n",
    "            print('Cannot find store header information, skipping')\n",
    "            self.store_id_num -= 1\n",
    "            return\n",
    "            \n",
    "        store_head_list = store_head.find_all('dl')\n",
    "        if len(store_head_list) < 2:\n",
    "            print('Store header information is incomplete, skipping')\n",
    "            self.store_id_num -= 1\n",
    "            return\n",
    "            \n",
    "        store_head_list = store_head_list[1].find_all('span')\n",
    "        if not store_head_list:\n",
    "            print('Cannot determine store type, skipping')\n",
    "            self.store_id_num -= 1\n",
    "            return\n",
    "        \n",
    "        # Check if it's a ramen or tsukemen restaurant\n",
    "        # if store_head_list[0].text not in {'ラーメン', 'つけ麺'}:\n",
    "        #     print('Not a ramen or tsukemen restaurant, skipping')\n",
    "        #     self.store_id_num -= 1\n",
    "        #     return\n",
    "        \n",
    "        # Get rating score\n",
    "        rating_score_tag = soup.find('b', class_='c-rating__val')\n",
    "        if not rating_score_tag or not rating_score_tag.span:\n",
    "            print('評価が見つかりません')\n",
    "            self.store_id_num -= 1\n",
    "            return\n",
    "            \n",
    "        rating_score = rating_score_tag.span.string\n",
    "        print('評価点数：{}点'.format(rating_score), end='')\n",
    "        self.score = rating_score\n",
    "        \n",
    "        # Skip restaurants with no rating\n",
    "        if rating_score == '-':\n",
    "            print('  評価がないため処理対象外')\n",
    "            self.store_id_num -= 1\n",
    "            return\n",
    "               \n",
    "        # This code skips restaurants with rating below 3.5\n",
    "        # Good for looking for good places\n",
    "        # if float(rating_score) < 3.5:\n",
    "        #     print('  食べログ評価が3.5未満のため処理対象外')\n",
    "        #     self.store_id_num -= 1\n",
    "        #     return\n",
    "        \n",
    "        # Get review count\n",
    "        review_tag_id = soup.find('li', id=\"rdnavi-review\")\n",
    "        if not review_tag_id or not review_tag_id.a:\n",
    "            print('  レビューセクションが見つかりません')\n",
    "            self.store_id_num -= 1\n",
    "            return\n",
    "            \n",
    "        review_tag = review_tag_id.a.get('href')\n",
    "        \n",
    "        # Get review count\n",
    "        review_count_span = review_tag_id.find('span', class_='rstdtl-navi__total-count')\n",
    "        if not review_count_span or not review_count_span.em:\n",
    "            print('  レビュー件数が見つかりません')\n",
    "            self.review_count = 0\n",
    "        else:\n",
    "            print('  レビュー件数：{}'.format(review_count_span.em.string), end='')\n",
    "            self.review_count = review_count_span.em.string\n",
    "        \n",
    "              # Get price information (daytime and nighttime)\n",
    "        # Find all price target elements\n",
    "        price_targets = soup.find_all('a', class_='rdheader-budget__price-target')\n",
    "        \n",
    "         # For each price target, determine if it's lunch or dinner\n",
    "        for price_target in price_targets:\n",
    "            # Find the closest time indicator (lunch or dinner icon)\n",
    "            parent_element = price_target.parent\n",
    "            while parent_element and not parent_element.find('i', class_='c-rating-v3__time'):\n",
    "                parent_element = parent_element.parent\n",
    "                \n",
    "            time_elem = parent_element.find('i', class_='c-rating-v3__time') if parent_element else None\n",
    "            \n",
    "            if time_elem and price_target:\n",
    "                price_text = price_target.text.strip()\n",
    "                \n",
    "                # Check class list for 'lunch' or 'dinner' substring\n",
    "                class_list = time_elem.get('class', [])\n",
    "                is_lunch = any('lunch' in class_name for class_name in class_list)\n",
    "                is_dinner = any('dinner' in class_name for class_name in class_list)\n",
    "                \n",
    "                # Assign price to appropriate category\n",
    "                if is_lunch:\n",
    "                    self.daytime_price, self.daytime_price_low, self.daytime_price_high = parse_price_range(price_text)\n",
    "                elif is_dinner:\n",
    "                    self.nighttime_price, self.nighttime_price_low, self.nighttime_price_high = parse_price_range(price_text)  \n",
    "            \n",
    "        # Display the extracted price information\n",
    "        day_info = f\"{self.daytime_price} ({self.daytime_price_low}-{self.daytime_price_high})\" if self.daytime_price else \"-\"\n",
    "        night_info = f\"{self.nighttime_price} ({self.nighttime_price_low}-{self.nighttime_price_high})\" if self.nighttime_price else \"-\"\n",
    "        print(f\"  価格帯：昼 {day_info}、夜 {night_info}\", end='')\n",
    "        \n",
    "        # Get the review list page URL and scrape the first page of reviews\n",
    "        page_num = 1  # Just scrape the first page of reviews\n",
    "        \n",
    "        if review_tag:\n",
    "            review_url = review_tag + 'COND-0/smp1/?lc=0&rvw_part=all&PG=' + str(page_num)\n",
    "            # Used after other information is obtained\n",
    "            print(' . ', end='')\n",
    "            self.scrape_review_page(review_url)\n",
    "    \n",
    "        self.make_df()\n",
    "        return\n",
    "\n",
    "    def scrape_review_page(self, review_url):\n",
    "        \"\"\"\n",
    "        Parse the review list page and extract metrics\n",
    "        \"\"\"\n",
    "        # Add a small delay before requesting review page\n",
    "        time.sleep(0.5)\n",
    "        \n",
    "        with urllib.request.urlopen(review_url) as r:\n",
    "            content = r.read()\n",
    "            status_code = r.status\n",
    "        if status_code != http.client.OK:\n",
    "            print(f'error: not found{ review_url }')\n",
    "            return False\n",
    "\n",
    "        soup = BeautifulSoup(content, 'html.parser')\n",
    "        review_items = soup.find_all('div', class_='rvw-item')\n",
    "        \n",
    "        if len(review_items) == 0:\n",
    "            return False\n",
    "        \n",
    "        total_photos = 0\n",
    "        total_likes = 0\n",
    "        total_bookmarks = 0\n",
    "        \n",
    "        for review_item in review_items:\n",
    "            # Count photos\n",
    "            photo_section = review_item.find('div', class_='rvw-photo')\n",
    "            if photo_section:\n",
    "                # Check for \"more photos\" indicator\n",
    "                more_photos = photo_section.find('span', class_='c-photo-more__num')\n",
    "                if more_photos:\n",
    "                    try:\n",
    "                        total_photos += int(more_photos.text)\n",
    "                    except ValueError:\n",
    "                        pass\n",
    "                \n",
    "                # Count visible photos\n",
    "                photo_list = photo_section.find_all('li', class_='rvw-photo__list-item')\n",
    "                if photo_list:\n",
    "                    total_photos += len(photo_list)\n",
    "            \n",
    "            # Count likes\n",
    "            like_count_elem = review_item.find('div', class_='js-like-source')\n",
    "            if like_count_elem:\n",
    "                try:\n",
    "                    like_data = re.search(r'\"count\":(\\d+)', like_count_elem.text)\n",
    "                    if like_data:\n",
    "                        total_likes += int(like_data.group(1))\n",
    "                except (ValueError, AttributeError):\n",
    "                    pass\n",
    "            \n",
    "            # Count bookmarks\n",
    "            bookmark_elem = review_item.find('div', class_='js-vote-interest')\n",
    "            if bookmark_elem and bookmark_elem.get('data-hozon-count'):\n",
    "                try:\n",
    "                    total_bookmarks += int(bookmark_elem.get('data-hozon-count'))\n",
    "                except ValueError:\n",
    "                    pass\n",
    "            \n",
    "        # Update counters\n",
    "        self.photo_count = total_photos\n",
    "        self.like_count = total_likes\n",
    "        self.bookmark_count = total_bookmarks\n",
    "        \n",
    "        print(f'  写真：{total_photos}枚、いいね：{total_likes}件、保存：{total_bookmarks}件')\n",
    "        return True\n",
    "    \n",
    "    def make_df(self):\n",
    "        \"\"\"\n",
    "        Create a DataFrame row with the collected data\n",
    "        \"\"\"\n",
    "        # Use 0-padding\n",
    "        self.store_id = str(self.store_id_num).zfill(8) \n",
    "        # Create row for the DataFrame\n",
    "        se = pd.Series([\n",
    "            self.store_id, \n",
    "            self.store_name, \n",
    "            self.score, \n",
    "            self.p_area, \n",
    "            self.genre,\n",
    "            self.review_count, \n",
    "            self.daytime_price,\n",
    "            self.daytime_price_low,\n",
    "            self.daytime_price_high,\n",
    "            self.nighttime_price,\n",
    "            self.nighttime_price_low,\n",
    "            self.nighttime_price_high,\n",
    "            self.photo_count, \n",
    "            self.like_count, \n",
    "            self.bookmark_count, \n",
    "        ], self.columns) \n",
    "        # Add the row to the DF\n",
    "        self.df = pd.concat([self.df, pd.DataFrame([se], columns=self.columns)], ignore_index=True)\n",
    "        # Increment the successful stores counter\n",
    "        self.successful_stores += 1\n",
    "        \n",
    "        # Reset values for next restaurant\n",
    "        self.photo_count = 0\n",
    "        self.like_count = 0\n",
    "        self.bookmark_count = 0\n",
    "        self.daytime_price = ''\n",
    "        self.daytime_price_low = None\n",
    "        self.daytime_price_high = None\n",
    "        self.nighttime_price = ''\n",
    "        self.nighttime_price_low = None\n",
    "        self.nighttime_price_high = None\n",
    "        return\n",
    "    \n",
    "def parse_price_range(price_text):\n",
    "    \"\"\"\n",
    "    Parse price ranges like \"￥1,000～￥1,999\" into low and high values\n",
    "    Returns a tuple of (original_text, low_value, high_value)\n",
    "    \"\"\"\n",
    "    if not price_text or price_text == '-':\n",
    "        return price_text, None, None\n",
    "        \n",
    "    # Regex the numbers from the price range\n",
    "    numbers = re.findall(r'￥([0-9,]+)', price_text)\n",
    "    \n",
    "    if len(numbers) >= 2:\n",
    "        # Convert to integers and remove commas\n",
    "        try:\n",
    "            low = int(numbers[0].replace(',', ''))\n",
    "            high = int(numbers[1].replace(',', ''))\n",
    "            return price_text, low, high\n",
    "        except ValueError:\n",
    "            pass\n",
    "    elif len(numbers) >= 1 and price_text.startswith(\"～￥\"):\n",
    "        # If price range doesn't start with a number, make lower bound 1 and use upper bound normally\n",
    "        low = 1\n",
    "        high = numbers[0]\n",
    "        return price_text, low, high\n",
    "    \n",
    "    return price_text, None, None"
   ],
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-04-11T20:18:20.936088Z",
     "start_time": "2025-04-11T20:18:20.890563Z"
    }
   },
   "id": "initial_id",
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Food categories: 15, Total URLs to process: 15\n"
     ]
    }
   ],
   "source": [
    "# Base URL for Tokyo restaurants\n",
    "tokyo_base_url = \"https://tabelog.com/tokyo/rstLst/\"\n",
    "# Categories of food\n",
    "food_categories = [\n",
    "    \"ramen\", \n",
    "    \"japanese\", \n",
    "    \"washoku\", \n",
    "    \"sushi\", \n",
    "    \"seafood\", \n",
    "    \"udon\", \n",
    "    \"yakiniku\", \n",
    "    \"curry\", \n",
    "    \"italian\", \n",
    "    \"izakaya\", \n",
    "    \"sweets\", \n",
    "    \"chinese\", \n",
    "    \"pizza\", \n",
    "    \"syabusyabu\", \n",
    "    \"korea\"\n",
    "]\n",
    "food_category_urls_tokyo = []\n",
    "for category in food_categories:\n",
    "    food_category_urls_tokyo.append(tokyo_base_url + category + \"/\")\n",
    "\n",
    "print(f\"Food categories: {len(food_categories)}, Total URLs to process: {len(food_category_urls_tokyo)}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-04-11T20:18:43.770846Z",
     "start_time": "2025-04-11T20:18:43.766001Z"
    }
   },
   "id": "531bbb1fe407e97",
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Progress bar\n",
    "progress = tqdm.tqdm(total=len(food_category_urls_tokyo))\n",
    "for category, tokyo_food_category_url in zip(food_categories, food_category_urls_tokyo):\n",
    "    print(f\"Processing category - {category}\")\n",
    "    output_csv_name = \"./data/food_category_data/tabelog_tokyo_\" + category + \"_data.csv\"\n",
    "    # Tokyo Ramen Rating Data, 20 entries per page - 10 pages → 200 entries \n",
    "    tabelog_tokyo_genre = Tabelog(tokyo_base_url, test_mode=False, p_area='東京都内', begin_page=1, end_page=10)\n",
    "    tabelog_tokyo_genre.df.to_csv(output_csv_name, encoding='utf-8-sig', index=False)\n",
    "    progress.update(1)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "71c3c7c559785452",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Use this if prices are missing for rows that start with \"～￥\" or have \"～￥999\" imputed\n",
    "for category in food_categories:\n",
    "    output_csv_name = \"./data/food_category_data/tabelog_tokyo_\" + category + \"_data.csv\"\n",
    "    tabelog_tokyo_df = pd.read_csv(output_csv_name, encoding='utf-8-sig')\n",
    "    tblg_df_copy = tabelog_tokyo_df.copy()\n",
    "    \n",
    "    # Find rows where price is \"～￥999\"\n",
    "    daytime_mask = tblg_df_copy['daytime_price'] == \"～￥999\"\n",
    "    nighttime_mask = tblg_df_copy['nighttime_price'] == \"～￥999\"\n",
    "    \n",
    "    # Impute missing values\n",
    "    tblg_df_copy.loc[daytime_mask, 'daytime_price_low'] = 1\n",
    "    tblg_df_copy.loc[daytime_mask, 'daytime_price_high'] = 999\n",
    "    tblg_df_copy.loc[nighttime_mask, 'nighttime_price_low'] = 1\n",
    "    tblg_df_copy.loc[nighttime_mask, 'nighttime_price_high'] = 999\n",
    "    \n",
    "    # Write CSV\n",
    "    tblg_df_copy.to_csv(output_csv_name, encoding='utf-8-sig', index=False)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-04-11T20:20:29.928372Z",
     "start_time": "2025-04-11T20:20:29.867219Z"
    }
   },
   "id": "db337e71e41d7a9",
   "execution_count": 9
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# for category in food_categories:\n",
    "#     output_csv_name = \"./data/food_category_data/tabelog_tokyo_\" + category + \"_data.csv\"\n",
    "#     tabelog_tokyo_df = pd.read_csv(output_csv_name, encoding='utf-8-sig')\n",
    "#     if \"Unnamed: 0.1\" in tabelog_tokyo_df.columns:\n",
    "#         print(\"Removing unnamed columns\")\n",
    "#         tabelog_tokyo_df = tabelog_tokyo_df.drop('Unnamed: 0.1', axis=1)\n",
    "#         tabelog_tokyo_df.to_csv(output_csv_name, encoding='utf-8-sig', index=False)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "38c17c67ed674cac",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f6f29ee2e58cd34"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Combine all CSV data into one dataset\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8c491264c5a5526a"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
